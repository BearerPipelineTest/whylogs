{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71f8605",
   "metadata": {},
   "source": [
    "# Data Monitoring with Flask and whylogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219a03d",
   "metadata": {},
   "source": [
    "\n",
    "In this example we will deploy a flask app with an ml model and use whylogs to monitor the data flowing throught the application. In this case we are using a pytorch image classification model based on densenet.\n",
    "\n",
    "\n",
    "The api will receive a json request with the path of the image file, this path can be locally acesseed by the app or could be an for example a `s3` bucket or another storage supported by `smart_open`. \n",
    "The response will be also a `json` with the classification of the image based on ImageNet pretrained model, hence it will be one of the ImageNet classes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dca82a",
   "metadata": {},
   "source": [
    "For this example you will need to install:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Flask==2.0.1 torchvision==0.10.0 whylogs==0.6.1 requests==2.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486a5be",
   "metadata": {},
   "source": [
    "We will begin by loading the model and the `whylogs` logger in memory as the application is launched. The function `get_or_create_session` will create a local configuration or load the default configuration file `.whylabs.yml` \n",
    "If you want to send your profiles whylabs you can send them directly to whylabs, witth the following enviroment variables. \n",
    "\n",
    "```python\n",
    "from torchvision import models\n",
    "\n",
    "from whylogs import get_or_create_session\n",
    "\n",
    "os.environ[\"WHYLABS_API_KEY\"] = \"<API-KEY>\"t\n",
    "os.environ[\"WHYLABS_API_ENDPOINT\"] = \"<end point override. not required>\"\n",
    "os.environ[\"WHYLABS_DEFAULT_ORG_ID\"] = \"<your-org-id>\"\n",
    "os.environ[\"WHYLABS_DEFAULT_DATASET_ID\"] = \"<your-default-dataset-id>\"\n",
    "\n",
    "session = get_or_create_session()\n",
    "whylog_logger = session.logger(tags={\"datasetId\": \"<override-dataset-id>\"}, , \n",
    "                        dataset_timestamp=datetime.datetime.now(datetime.timezone.utc), \n",
    "                        with_rotation_time=\"30s\")\n",
    "\n",
    "imagenet_class_index = json.load(open(\"imagenet_class_index.json\"))\n",
    "model = models.densenet121(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "app = Flask(__name__)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590499f",
   "metadata": {},
   "source": [
    "## Receiving Request and Monitoring Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed19bb",
   "metadata": {},
   "source": [
    "We will set the endpoint for our app to be `/predict`, this will the main interactions with our application, hence it is important to monitor all the data that is coming in. In this case the request json will include a path to an image.\n",
    "\n",
    "\n",
    "```python\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        filepath=request.json.get(\"file\",None) \n",
    "        image = fetch_image(filepath)\n",
    "        \n",
    "        whylog_logger.log({\"filepath\":filepath })\n",
    "        whylog_logger.log_image(image)\n",
    "        \n",
    "       \n",
    "```\n",
    "\n",
    "whylogs will aggregate the statistics for the data that is comming in at every hour, saving it locally or sending it to WhyLabs for automated monitoring.  Allowing you to monitoring changes to the request being made or images being loaded.\n",
    "\n",
    "### Data preparation \n",
    "\n",
    "To get your data to how the model expects it we need to resize, normalize it and converted to pytorch tensor type.\n",
    "We will use the transform lib found withi torchvision. We could additionally log the transform data.\n",
    "\n",
    "```python\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def transform_image(image):\n",
    "    my_transforms = transforms.Compose([transforms.Resize(255),\n",
    "                                        transforms.CenterCrop(224),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(\n",
    "                                            [0.485, 0.456, 0.406],\n",
    "                                            [0.229, 0.224, 0.225])])\n",
    "    return my_transforms(image).unsqueeze(0)\n",
    "```\n",
    "### Inference\n",
    "Putting it all together, we add the additional steps for the model inference and log the output info. \n",
    "\n",
    "```python\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "\n",
    "    if request.method == \"POST\":\n",
    "        filepath=request.json.get(\"file\",None) \n",
    "        image = fetch_image(filepath)\n",
    "\n",
    "        whylog_logger.log({\"filepath\":filepath })\n",
    "        whylog_logger.log_image(image)\n",
    "    \n",
    "        tensor = transform_image(image)\n",
    "\n",
    "        whylog_logger.log({\"batch_size\": tensor.shape[0]})\n",
    "        \n",
    "        outputs = model.forward(tensor)\n",
    "        conf, p_index = outputs.max(1)\n",
    "        predicted_idx = str(p_index.item())\n",
    "        class_id, class_name= imagenet_class_index[predicted_idx]\n",
    "        \n",
    "        whylog_logger.log({\"confidence\": conf.item()})\n",
    "        whylog_logger.log({\"class_id\": class_id})\n",
    "        whylog_logger.log({\"class_name\": class_name})\n",
    "\n",
    "        return jsonify({\"class_id\": class_id, \"class_name\": class_name}) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d936f22d",
   "metadata": {},
   "source": [
    "You can test run your app by running locally with \n",
    "\n",
    "```bash\n",
    "$~ python my_flask_app.py\n",
    "```\n",
    "and can use the snipppet below to make requests to the app. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b07466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from torchvision import transforms\n",
    "\n",
    "resp = requests.post(\"http://localhost:5000/predict\",\n",
    "                     json={\"file\": '../data/flower2.jpg'})\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d11cf3",
   "metadata": {},
   "source": [
    "You can wait 30 seconds until the log rotates, or you can change the log rotation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30d1f37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/my_deployed_model/e5f8620a-8f15-4d41-a0e9-e9845c317377/protobuf/dataset_profile.2021-08-25_13-15-22.bin',\n",
       " 'output/my_deployed_model/e5f8620a-8f15-4d41-a0e9-e9845c317377/protobuf/dataset_profile.2021-08-25_13-15-52.bin',\n",
       " 'output/my_deployed_model/e5f8620a-8f15-4d41-a0e9-e9845c317377/protobuf/dataset_profile.2021-08-25_13-16-22.bin',\n",
       " 'output/my_deployed_model/e5f8620a-8f15-4d41-a0e9-e9845c317377/protobuf/dataset_profile.2021-08-25_13-16-52.bin']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import glob \n",
    "\n",
    "time.sleep(30)\n",
    "files=glob.glob(\"output/*/*/*/*.bin\")\n",
    "sorted(files)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8113f",
   "metadata": {},
   "source": [
    "You can load your profiles and view them individually with the viewer, or with the some of viz tools in the library.\n",
    "Or even better send them to WhyLabs where you can see distributional shifts, anomaly detection and much more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2612d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs import DatasetProfile\n",
    "profiles=DatasetProfile.read_protobuf(files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6b28f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whylogs.viz import profile_viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9aa467fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/var/folders/pr/f715zv8x17b1v5vwydgv2gq40000gq/T/tmpr3be6_r7.html'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile_viewer(profiles=[profile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2504c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whylogs-dev",
   "language": "python",
   "name": "whylogs-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
